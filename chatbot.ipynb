{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f9ec5ec",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c976d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necess√°rio AWS CLI 2.13.0+\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "env_vars = !cat .env\n",
    "\n",
    "for var in env_vars:\n",
    "    key, value = var.split(\"=\", 1)\n",
    "    os.environ[key] = value.replace('\"', \"\")\n",
    "\n",
    "\n",
    "# Cria o cliente Bedrock\n",
    "client = boto3.client(service_name='bedrock-runtime', region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a532960e",
   "metadata": {},
   "source": [
    "# v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8753bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(prompt: str):\n",
    "    return json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 200,  # Token m√°ximo para a resposta\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                # definir melhor o prompt \n",
    "                \"content\": f\"Human: {prompt}\"\n",
    "                \"Assistant: Forne√ßa uma resposta com m√°ximo 300 caracteres,\"\n",
    "                \" ideal para um e-commerce de roupas e itens de vesti√°rio. N√£o mencionar instru√ß√µes do prompt na resposta. \"\n",
    "                \"Assistant:\" \n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.5,  # Controla a aleatoriedade da resposta\n",
    "        \"top_k\": 250,        # Controla a diversidade da resposta\n",
    "        \"top_p\": 0.2         # Controla a diversidade acumulada da resposta\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8870e197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistente: Ol√°! Sou seu Assistente Virtual de Moda. \n",
      "Como posso ajudar voc√™ hoje?\n",
      "User: Me indique um sand√°lia para usar na praia? \n",
      "Assistente: Para a praia, recomendo chinelos de dedo de borracha, como as tradicionais havaianas. S√£o pr√°ticos, resistentes √† √°gua salgada, secam rapidamente e protegem os p√©s do sol quente. Modelos coloridos ou em tons neutros combinam com biqu√≠nis e sa√≠das de praia.\n",
      "\n",
      "User: Para usar no shopping? \n",
      "Assistente: Para o shopping, sugiro um look confort√°vel e estiloso: cal√ßa jeans ou legging, blusa de malha leve, t√™nis branco ou slip on, e uma jaqueta ou casaco para eventuais varia√ß√µes de temperatura. Bolsa transversal completa o visual pr√°tico.\n",
      "\n",
      "User: sair\n",
      "Assistente: At√© logo! Se precisar de mais ajuda, estarei aqui.\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = 'us.anthropic.claude-3-5-haiku-20241022-v1:0'\n",
    "print(\"Assistente: Ol√°! Sou seu Assistente Virtual de Moda. \\n\"\n",
    "      \"Como posso ajudar voc√™ hoje?\"\n",
    ")\n",
    "\n",
    "while True:\n",
    "    input_user = input()\n",
    "    print(f\"User: {input_user}\")\n",
    "    if input_user.lower() in [\"sair\", \"exit\", \"quit\"]:\n",
    "        print(\"Assistente: At√© logo! Se precisar de mais ajuda, estarei aqui.\")\n",
    "        break\n",
    "    response = client.invoke_model(\n",
    "        body=get_config(input_user),\n",
    "        modelId=MODEL_ID,\n",
    "        accept='application/json',\n",
    "        contentType='application/json'\n",
    "    )\n",
    "    response_payload = json.loads(response['body'].read().decode('utf-8'))\n",
    "    answer = response_payload['content'][0]['text']\n",
    "    print(f\"Assistente: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27ad13e",
   "metadata": {},
   "source": [
    "# V2 - Hist√≥rico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5640dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = []\n",
    "\n",
    "def get_history():\n",
    "    return \"\\n\".join(history)\n",
    "\n",
    "def get_config(input: str):\n",
    "    prompt = (\n",
    "        get_history() + \"\\n\"\n",
    "        f\"Human: {input}\"\n",
    "        \"Assistant: Forne√ßa uma resposta com m√°ximo 300 caracteres,\"\n",
    "        \" ideal para um e-commerce de roupas e itens de vesti√°rio. N√£o mencionar instru√ß√µes do prompt na resposta. \"\n",
    "        \"Assistant:\" \n",
    "    )\n",
    "    return json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 200,  # Token m√°ximo para a resposta\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                # definir melhor o prompt \n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.5,  # Controla a aleatoriedade da resposta\n",
    "        \"top_k\": 250,        # Controla a diversidade da resposta\n",
    "        \"top_p\": 0.2         # Controla a diversidade acumulada da resposta\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a6e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistente: Ol√°! Sou seu Assistente Virtual de Moda. \n",
      "Como posso ajudar voc√™ hoje?\n",
      "User: Me indique um sand√°lia para usar na praia\n",
      "Assistente: Chinelo Havaianas Top: modelo cl√°ssico, confort√°vel e resistente √† √°gua, ideal para praia. Dispon√≠vel em v√°rias cores, feito de borracha, antiderrapante e leve. Protege os p√©s do sol e areia, sendo uma escolha pr√°tica e estilosa para dias de sol.\n",
      "\n",
      "User: e no shoppping? \n",
      "Assistente: Sand√°lia Papete Feminina: modelo vers√°til em couro ou material sint√©tico, com tiras largas e sola confort√°vel. Combina eleg√¢ncia e praticidade, perfeita para looks casuais no shopping. Dispon√≠vel em cores neutras como preto, nude e branco.\n",
      "\n",
      "User: sair\n",
      "Assistente: At√© logo! Se precisar de mais ajuda, estarei aqui.\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "MODEL_ID = 'us.anthropic.claude-3-5-haiku-20241022-v1:0'\n",
    "print(\"Assistente: Ol√°! Sou seu Assistente Virtual de Moda. \\n\"\n",
    "      \"Como posso ajudar voc√™ hoje?\"\n",
    ")\n",
    "\n",
    "while True:\n",
    "    input_user = input()\n",
    "    history.append(f\"Human: {input_user}\")\n",
    "    print(f\"User: {input_user}\")\n",
    "    if input_user.lower() in [\"sair\", \"exit\", \"quit\"]:\n",
    "        print(\"Assistente: At√© logo! Se precisar de mais ajuda, estarei aqui.\")\n",
    "        break\n",
    "    response = client.invoke_model(\n",
    "        body=get_config(input_user),\n",
    "        modelId=MODEL_ID,\n",
    "        accept='application/json',\n",
    "        contentType='application/json'\n",
    "    )\n",
    "    response_payload = json.loads(response['body'].read().decode('utf-8'))\n",
    "    answer = response_payload['content'][0]['text']\n",
    "    history.append(f\"Assistant: {answer}\")\n",
    "    print(f\"Assistente: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e50f7f6",
   "metadata": {},
   "source": [
    "# V3 Langchain\n",
    "Framework open source, possui libs que facilita para o desenvolvimento em IA  (deixa o c√≥digo mais eficiente)\n",
    "\n",
    "**LangChain** facilita a cria√ß√£o de uma camada de abstra√ß√£o, o que simplifica a cria√ß√£o e o gerenciamento dos prompts, resultando em um assistente mais modular e f√°cil de manter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ccb7613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain langchain-community langchain-aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "141442e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "bedrock_client = boto3.client(service_name='bedrock-runtime', region_name='us-east-1')\n",
    "model = ChatBedrock(\n",
    "    client=bedrock_client,\n",
    "    model_id='us.anthropic.claude-3-5-haiku-20241022-v1:0',\n",
    "    model_kwargs={\"temperature\": 0.5, \"max_tokens\": 300}\n",
    ")\n",
    "\n",
    "history = []\n",
    "\n",
    "def get_history():\n",
    "    return \"\\n\".join(history)\n",
    "\n",
    "def get_config(input: str):\n",
    "    return ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Voc√™ √© um assistente virtual especializado em moda para e-commerce. Forne√ßa respostas concisas e relevantes com m√°ximo 300 caracteres.\"),\n",
    "        (\"human\", get_history() + \"\\n\" + input)\n",
    "    ])\n",
    "\n",
    "# Encapsulamento (Langchain): Fun√ß√£o para invocar o modelo com o prompt formatado (uma cadeia para facilitar o uso do modelo)\n",
    "def inv_model(prompt: str):\n",
    "    chat_prompt = get_config(prompt)\n",
    "    response = model.invoke(chat_prompt.format_messages())\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fb8d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistente: Ol√°! Sou seu Assistente Virtual de Moda. \n",
      "Como posso ajudar voc√™ hoje?\n",
      "User: Pode me indicar sand√°lia idela para usar na praia\n",
      "Assistente: Para praia, recomendo chinelos de borracha ou papete havaianas. S√£o confort√°veis, resistentes √† √°gua, secam r√°pido e protegem os p√©s do sol quente. Modelos coloridos e pr√°ticos s√£o ideais.\n",
      "\n",
      "User: e para ir no shopping? \n",
      "Assistente: Para o shopping, sugiro sand√°lias rasteiras de couro, papetes elegantes ou sapatilhas confort√°veis. Priorize modelos vers√°teis que combinem com diferentes looks casuais.\n",
      "\n",
      "User: sair\n",
      "Assistente: At√© logo! Se precisar de mais ajuda, estarei aqui.\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "MODEL_ID = 'us.anthropic.claude-3-5-haiku-20241022-v1:0'\n",
    "print(\"Assistente: Ol√°! Sou seu Assistente Virtual de Moda. \\n\"\n",
    "      \"Como posso ajudar voc√™ hoje?\"\n",
    ")\n",
    "\n",
    "while True:\n",
    "    input_user = input()\n",
    "    history.append(f\"Human: {input_user}\")\n",
    "    print(f\"User: {input_user}\")\n",
    "    if input_user.lower() in [\"sair\", \"exit\", \"quit\"]:\n",
    "        print(\"Assistente: At√© logo! Se precisar de mais ajuda, estarei aqui.\")\n",
    "        break\n",
    "    response = inv_model(input_user)\n",
    "    answer = response\n",
    "    history.append(f\"Assistant: {answer}\")\n",
    "    print(f\"Assistente: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea558cee",
   "metadata": {},
   "source": [
    "# V4 - RAG (Retrieval-augmented generation) \n",
    "No processo de gerar uma resposta o LLM consulta uma base para responder\n",
    "Combina a capacidade do modelos (LLMs) gerar texto com a recupera√ß√£o de informa√ß√£o externa. \n",
    "No RAG consegue fazer o modelo se utilize de recursos para responder coisas em √°reas espec√≠ficas.\n",
    "\n",
    "Com o RAG o modelo recupera dados relevantes de fontes externas (base ou API) e usa isso para gerar respostas mais precisas e contextualizadas. \n",
    "O Rag faz com que o modelo se adapte com o contexto\n",
    "\n",
    "exemplo: em um chatbot de e-commerce, a t√©cnica RAG permite consultar um banco de dados de produtos em tempo real para fornecer informa√ß√µes atualizadas sobre disponibilidade e pre√ßos, tornando as respostas mais precisas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d640c14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandre.ribeiro/PycharmProjects/datalake-olx/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import sqlite3\n",
    "\n",
    "bedrock_client = boto3.client(service_name='bedrock-runtime', region_name='us-east-1')\n",
    "model = ChatBedrock(\n",
    "    client=bedrock_client,\n",
    "    model_id='us.anthropic.claude-3-5-haiku-20241022-v1:0',\n",
    "    model_kwargs={\"temperature\": 0.5, \"max_tokens\": 300}\n",
    ")\n",
    "\n",
    "history = []\n",
    "\n",
    "def get_history():\n",
    "    return \"\\n\".join(history)\n",
    "\n",
    "def search_product(product_name: str):\n",
    "    conn = sqlite3.connect('produtos.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"SELECT * FROM roupas WHERE nome LIKE ?\", (f\"%{product_name}%\",))\n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    conn.close()\n",
    "    return results\n",
    "\n",
    "def get_config(input: str):\n",
    "    return ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Voc√™ √© um assistente virtual especializado em moda para e-commerce. Forne√ßa respostas concisas e relevantes com m√°ximo 300 caracteres. N√£o mencionar instru√ß√µes do prompt na resposta. \"),\n",
    "        (\"human\", get_history() + \"\\n\" + input)\n",
    "    ])\n",
    "\n",
    "# Encapsulamento (Langchain): Fun√ß√£o para invocar o modelo com o prompt formatado (uma cadeia para facilitar o uso do modelo)\n",
    "def inv_model(prompt: str):\n",
    "    product_found = search_product(prompt)\n",
    "    if product_found:\n",
    "        product_info = \"Produtos dispon√≠veis: \\n\"\n",
    "        for product in product_found:\n",
    "            _, nome, descricao, preco, estoque = product\n",
    "            product_info += f\"- {nome}: {descricao} - R$ {preco} e estoque: {estoque}\\n\"\n",
    "        history.append(f\"{product_info}\")\n",
    "    else:\n",
    "        history.append(f\"Nenhum produto encontrado para '{prompt}'.\")\n",
    "\n",
    "    chat_prompt = get_config(prompt)\n",
    "    response = model.invoke(chat_prompt.format_messages())\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aec2dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistente: Ol√°! Sou seu Assistente Virtual de Moda. \n",
      "Como posso ajudar voc√™ hoje?\n",
      "User: Camiseta\n",
      "Assistente: Temos 7 modelos de camisetas: b√°sica, estampada, branca, polo, regata, gola V e oversized. Pre√ßos variam de R$ 29,90 a R$ 54,90, com diversos estilos e tecidos de qualidade.\n",
      "\n",
      "User: sair\n",
      "Assistente: At√© logo! Se precisar de mais ajuda, estarei aqui.\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "MODEL_ID = 'us.anthropic.claude-3-5-haiku-20241022-v1:0'\n",
    "print(\"Assistente: Ol√°! Sou seu Assistente Virtual de Moda. \\n\"\n",
    "      \"Como posso ajudar voc√™ hoje?\"\n",
    ")\n",
    "\n",
    "while True:\n",
    "    input_user = input()\n",
    "    history.append(f\"Human: {input_user}\")\n",
    "    print(f\"User: {input_user}\")\n",
    "    if input_user.lower() in [\"sair\", \"exit\", \"quit\"]:\n",
    "        print(\"Assistente: At√© logo! Se precisar de mais ajuda, estarei aqui.\")\n",
    "        break\n",
    "    response = inv_model(input_user)\n",
    "    answer = response\n",
    "    history.append(f\"Assistant: {answer}\")\n",
    "    print(f\"Assistente: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a801d05f",
   "metadata": {},
   "source": [
    "# V5 - Refinar o prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8374f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import sqlite3\n",
    "\n",
    "bedrock_client = boto3.client(service_name='bedrock-runtime', region_name='us-east-1')\n",
    "\n",
    "def setup_model():\n",
    "    return ChatBedrock(\n",
    "        client=bedrock_client,\n",
    "        model_id='us.anthropic.claude-3-5-haiku-20241022-v1:0',\n",
    "        model_kwargs={\"temperature\": 0.5, \"max_tokens\": 300}\n",
    "    )\n",
    "\n",
    "model = setup_model()\n",
    "\n",
    "history = []\n",
    "\n",
    "def get_history():\n",
    "    return \"\\n\".join(history)\n",
    "\n",
    "def search_product(product_name: str):\n",
    "    conn = sqlite3.connect('produtos.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"SELECT * FROM roupas WHERE nome LIKE ?\", (f\"%{product_name}%\",))\n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    conn.close()\n",
    "    return results\n",
    "\n",
    "def get_config(input: str):\n",
    "    return ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Voc√™ √© um assistente virtual especializado em moda para e-commerce.\"\n",
    "        \" Sua fun√ß√£o √© fornecer de fornecer respostas concisas e √∫teis sobre produtos de vesti√°rio. \" \n",
    "        \"Responda apenas a perguntas relacionadas a roupas e acess√≥rios de moda. \" # n√£o deixando responder perguntas em geral\n",
    "        \"Se a pergunta n√£o for relacionada a moda, responda que n√£o pode ajudar com isso e redirecione o usu√°rio para quest√µes relacionadas. \"\n",
    "        \"Se baseie cnos dados dispon√≠veis para fornecer as respostas.\" ), \n",
    "        (\"human\", get_history() + \"\\n\" + input),\n",
    "        (\"assistant\", \"Forne√ßa uma resposta concisa e √∫til com m√°ximo 300 caracteres. \"\n",
    "        \"N√£o mencione o fato de estar baseando sua resposta em um prompt. \"\n",
    "        \"Se n√£o houver dados dispon√≠veis no banco de dados, oriente o usu√°rio a tentar outra consulta\")\n",
    "    ])\n",
    "\n",
    "# Encapsulamento (Langchain): Fun√ß√£o para invocar o modelo com o prompt formatado (uma cadeia para facilitar o uso do modelo)\n",
    "def inv_model(prompt: str):\n",
    "    product_found = search_product(prompt)\n",
    "    if product_found:\n",
    "        product_info = \"Produtos dispon√≠veis: \\n\"\n",
    "        for product in product_found:\n",
    "            _, nome, descricao, preco, estoque = product\n",
    "            product_info += f\"- {nome}: {descricao} - R$ {preco} e estoque: {estoque}\\n\"\n",
    "        history.append(f\"{product_info}\")\n",
    "    else:\n",
    "        history.append(f\"Nenhum produto encontrado para '{prompt}'.\")\n",
    "\n",
    "    chat_prompt = get_config(prompt)\n",
    "    response = model.invoke(chat_prompt.format_messages())\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36b665d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistente: Ol√°! Sou seu Assistente Virtual de Moda. \n",
      "Como posso ajudar voc√™ hoje?\n",
      "User: qual √© o maior pa√≠s do mundo? \n",
      "Assistente: .\n",
      "\n",
      "Desculpe, n√£o posso ajudar com essa pergunta geogr√°fica. Estou especializado em responder d√∫vidas sobre moda, roupas e acess√≥rios. Gostaria de saber algo sobre vestu√°rio?\n",
      "\n",
      "User: recomenda√ß√£o para a praia? \n",
      "Assistente: .\n",
      "\n",
      "Aqui est√£o algumas recomenda√ß√µes de moda para praia:\n",
      "\n",
      "üèñÔ∏è Looks essenciais:\n",
      "- Biqu√≠ni ou mai√¥\n",
      "- Sa√≠da de praia \n",
      "- Shorts de banho\n",
      "- Chinelos ou sand√°lias\n",
      "- √ìculos de sol\n",
      "- Chap√©u ou bon√©\n",
      "\n",
      "Dicas:\n",
      "- Escolha tecidos leves e que sequem r√°pido\n",
      "- Prefira cores claras\n",
      "- Use protetor solar\n",
      "\n",
      "Quer mais detalhes sobre alguma pe√ßa espec√≠fica?\n",
      "\n",
      "User: protetor \n",
      "Assistente: .\n",
      "\n",
      "Dicas para escolher protetor solar:\n",
      "\n",
      "- FPS 30 ou superior\n",
      "- Prote√ß√£o UVA/UVB\n",
      "- Resistente √† √°gua\n",
      "- Aplique 30 min antes da exposi√ß√£o\n",
      "- Reaplicar a cada 2 horas\n",
      "- Protege contra queimaduras e envelhecimento precoce\n",
      "\n",
      "Quer saber mais sobre prote√ß√£o solar?\n",
      "\n",
      "User: sair\n",
      "Assistente: At√© logo! Se precisar de mais ajuda, estarei aqui.\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "MODEL_ID = 'us.anthropic.claude-3-5-haiku-20241022-v1:0'\n",
    "print(\"Assistente: Ol√°! Sou seu Assistente Virtual de Moda. \\n\"\n",
    "      \"Como posso ajudar voc√™ hoje?\"\n",
    ")\n",
    "\n",
    "while True:\n",
    "    input_user = input()\n",
    "    history.append(f\"Human: {input_user}\")\n",
    "    print(f\"User: {input_user}\")\n",
    "    if input_user.lower() in [\"sair\", \"exit\", \"quit\"]:\n",
    "        print(\"Assistente: At√© logo! Se precisar de mais ajuda, estarei aqui.\")\n",
    "        break\n",
    "    response = inv_model(input_user)\n",
    "    answer = response\n",
    "    history.append(f\"Assistant: {answer}\")\n",
    "    print(f\"Assistente: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a332113",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
